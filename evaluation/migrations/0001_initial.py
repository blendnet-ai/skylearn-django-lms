# Generated by Django 4.2.16 on 2025-02-03 08:36

import datetime
from django.conf import settings
import django.contrib.postgres.fields
from django.db import migrations, models
import django.db.models.deletion
import uuid


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('OpenAIService', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='AssessmentAttempt',
            fields=[
                ('assessment_id', models.AutoField(primary_key=True, serialize=False)),
                ('attempted_list', models.JSONField(default=list)),
                ('question_list', models.JSONField(default=list)),
                ('type', models.IntegerField(blank=True, choices=[('0', 'Aptitude Test [Accenture]'), ('1', 'Communication Skills'), ('2', 'Psychometric Assessment'), ('3', 'Coding Assessment'), ('4', 'DSA Practice'), ('5', 'Mock Behavioural'), ('6', 'LSRW')], null=True)),
                ('last_saved', models.CharField(blank=True, max_length=255, null=True)),
                ('last_saved_section', models.IntegerField(blank=True, choices=[('0', 'Reading Comprehension'), ('1', 'Writing'), ('2', 'Speaking'), ('3', 'Listening'), ('4', 'Numerical'), ('5', 'Verbal'), ('6', 'Non Verbal'), ('7', 'Data Interpretation'), ('8', 'Quantitative'), ('9', 'Easy'), ('10', 'Medium'), ('11', 'Hard'), ('12', 'Basic')], null=True)),
                ('status', models.IntegerField(choices=[('0', 'Not Started'), ('1', 'In Progress'), ('2', 'Completed'), ('3', 'Evaluation Pending'), ('4', 'Abandoned')], default='0')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
                ('evaluation_triggered', models.BooleanField(default=False)),
                ('start_time', models.DateTimeField(blank=True, null=True)),
                ('test_duration', models.DurationField()),
                ('closed', models.BooleanField(default=False)),
                ('eval_data', models.JSONField(default=dict, null=True)),
                ('assessment_url', models.TextField(blank=True, null=True)),
                ('report_id', models.CharField(blank=True, max_length=255, null=True)),
                ('mode', models.IntegerField(choices=[('0', 'Evaluation'), ('1', 'Practice')], default='0')),
            ],
        ),
        migrations.CreateModel(
            name='AssessmentGenerationConfig',
            fields=[
                ('enabled', models.BooleanField(default=True)),
                ('assessment_generation_id', models.AutoField(primary_key=True, serialize=False)),
                ('kwargs', models.JSONField(default=dict)),
                ('assessment_generation_class_name', models.CharField(max_length=255)),
                ('evaluator_class_name', models.CharField(max_length=255)),
                ('assessment_name', models.CharField(max_length=255, unique=True)),
                ('assessment_display_name', models.CharField(max_length=255)),
                ('number_of_attempts', models.IntegerField(default=2)),
                ('assessment_type', models.IntegerField(choices=[('0', 'Qualitative'), ('1', 'Quantitative')], default='0')),
                ('test_duration', models.DurationField()),
                ('display_data', models.JSONField(default=dict)),
                ('start_date', models.DateTimeField(blank=True, null=True)),
                ('end_date', models.DateTimeField(blank=True, null=True)),
                ('due_date', models.DateTimeField(blank=True, null=True)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
            ],
        ),
        migrations.CreateModel(
            name='DSASheetsConfig',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(unique=True)),
                ('question_ids', models.JSONField(default=list)),
            ],
        ),
        migrations.CreateModel(
            name='EventFlow',
            fields=[
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='Date created')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='Date updated')),
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, help_text='Unique ID', primary_key=True, serialize=False)),
                ('type', models.CharField(max_length=24)),
                ('root_arguments', models.JSONField(blank=True, help_text='Arguments passed in while starting the event flow.', null=True)),
                ('status', models.IntegerField(choices=[(1, 'Started'), (2, 'Completed'), (3, 'Error'), (4, 'Aborted')], default=1)),
                ('initiated_by', models.CharField(editable=False, help_text='Id of the initiator event/person', max_length=64)),
                ('run_duration', models.DurationField(blank=True, null=True, verbose_name='Run duration')),
                ('start_time', models.DateTimeField(auto_now_add=True, null=True, verbose_name='Start time of event flow')),
                ('end_time', models.DateTimeField(blank=True, null=True, verbose_name='Start time of event flow')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='Question',
            fields=[
                ('id', models.AutoField(primary_key=True, serialize=False)),
                ('answer_type', models.IntegerField(blank=True, choices=[('0', 'MCQ'), ('1', 'MMCQ'), ('2', 'Subjective'), ('3', 'Voice')], null=True)),
                ('question_data', models.JSONField(default=dict)),
                ('markdown_question', models.TextField(blank=True, null=True)),
                ('level', models.IntegerField(blank=True, default=1, null=True)),
                ('audio_url', models.URLField(blank=True, max_length=500, null=True)),
                ('tags', models.JSONField(default=list)),
                ('category', models.IntegerField(blank=True, choices=[('0', 'Aptitude Test [Accenture]'), ('1', 'Communication Skills'), ('2', 'Psychometric Assessment'), ('3', 'Coding Assessment'), ('4', 'DSA Practice'), ('5', 'Mock Behavioural')], null=True)),
                ('sub_category', models.IntegerField(blank=True, choices=[('0', 'Reading Comprehension'), ('1', 'Writing'), ('2', 'Speaking'), ('3', 'Listening'), ('4', 'Numerical'), ('5', 'Verbal'), ('6', 'Non Verbal'), ('7', 'Data Interpretation'), ('8', 'Quantitative'), ('9', 'Easy'), ('10', 'Medium'), ('11', 'Hard'), ('12', 'Basic')], null=True)),
                ('time_required', models.DurationField(default=datetime.timedelta(seconds=60))),
                ('source', models.URLField(blank=True, null=True)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
            ],
        ),
        migrations.CreateModel(
            name='UserAttemptResponseEvaluation',
            fields=[
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='Date created')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='Date updated')),
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, help_text='Unique ID', primary_key=True, serialize=False)),
                ('score', models.DecimalField(blank=True, decimal_places=2, help_text='Cummulative score of evaluation', max_digits=4, null=True)),
                ('fluency', models.CharField(blank=True, help_text='Fluency score', max_length=32, null=True)),
                ('fluency_details', models.JSONField(blank=True, help_text='Additional fluency evaluation data', null=True)),
                ('pronunciation', models.CharField(blank=True, help_text='Pronunciation score', max_length=32, null=True)),
                ('pronunciation_details', models.JSONField(blank=True, help_text='Additional pronunciation evaluation data', null=True)),
                ('coherence', models.CharField(blank=True, help_text='Coherence score', max_length=32, null=True)),
                ('coherence_details', models.JSONField(blank=True, help_text='Additional coherence evaluation data', null=True)),
                ('grammar', models.CharField(blank=True, help_text='Grammar score', max_length=32, null=True)),
                ('grammar_details', models.JSONField(blank=True, help_text='Additional Grammar evaluation data', null=True)),
                ('vocab', models.CharField(blank=True, help_text='Vocab score', max_length=32, null=True)),
                ('vocab_details', models.JSONField(blank=True, help_text='Additional vocab evaluation data', null=True)),
                ('sentiment', models.CharField(blank=True, help_text='Sentiment score', max_length=32, null=True)),
                ('sentiment_details', models.JSONField(blank=True, help_text='Additional sentiment evaluation data', null=True)),
                ('summary', models.JSONField(blank=True, help_text='Any additional metadata related to the evaluation', null=True)),
                ('status', models.IntegerField(choices=[(1, 'Partial'), (2, 'Complete'), (3, 'Error')], default=1)),
                ('ideal_response_details', models.JSONField(blank=True, help_text='Metadata for ideal response', null=True)),
            ],
            options={
                'verbose_name': 'User Attempt Response Evaluation',
                'verbose_name_plural': 'User Attempt Response Evaluations',
            },
        ),
        migrations.CreateModel(
            name='QuestionIssues',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('type_of_issue', models.IntegerField(choices=[('0', 'Software Releated Issue'), ('1', 'Content Releated Issue'), ('2', 'Others Issue')])),
                ('description', models.TextField()),
                ('assessment_attempt', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='evaluation.assessmentattempt')),
                ('question', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='evaluation.question')),
                ('user_id', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),
            ],
        ),
        migrations.CreateModel(
            name='EventFlowProcessorState',
            fields=[
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='Date created')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='Date updated')),
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, help_text='Unique ID', primary_key=True, serialize=False)),
                ('processor_name', models.CharField(help_text='Processor name', max_length=32)),
                ('result', models.JSONField(blank=True, help_text='All the result of the task.', null=True)),
                ('error', models.JSONField(blank=True, help_text='Error stack trace of the task', null=True)),
                ('retriable_error', models.JSONField(blank=True, help_text='Error stack trace of the task, only in case of a retriable error. Last error occurred will be stored.', null=True)),
                ('status', models.IntegerField(choices=[(1, 'Pending'), (2, 'In Progress'), (3, 'Completed'), (4, 'Error'), (5, 'Completed With Error'), (6, 'Aborted'), (7, 'Retriable Error')], default=1)),
                ('run_duration', models.DurationField(blank=True, null=True, verbose_name='Run duration')),
                ('start_time', models.DateTimeField(blank=True, null=True, verbose_name='Start time of event flow')),
                ('end_time', models.DateTimeField(blank=True, null=True, verbose_name='Start time of event flow')),
                ('event_flow', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='processors', to='evaluation.eventflow')),
            ],
        ),
        migrations.CreateModel(
            name='DSAPracticeChatData',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('user_id', models.CharField(max_length=100)),
                ('chat_history', models.JSONField(default=list)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
                ('chat_count', models.IntegerField(default=0)),
                ('compile_duration_logs', models.JSONField(blank=True, default=list, help_text='Additional logs for monitoring purpose', null=True)),
                ('submit_compile_log', models.JSONField(blank=True, default=dict, help_text='Log of submission compilation', null=True)),
                ('assessment_attempt', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='evaluation.assessmentattempt')),
                ('chat_history_obj', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to='OpenAIService.chathistory')),
                ('question', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='evaluation.question')),
            ],
        ),
        migrations.AddField(
            model_name='assessmentattempt',
            name='assessment_generation_config_id',
            field=models.ForeignKey(on_delete=django.db.models.deletion.DO_NOTHING, to='evaluation.assessmentgenerationconfig'),
        ),
        migrations.AddField(
            model_name='assessmentattempt',
            name='user_id',
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL),
        ),
        migrations.CreateModel(
            name='UserEvalQuestionAttempt',
            fields=[
                ('id', models.AutoField(primary_key=True, serialize=False)),
                ('section', models.CharField(blank=True, max_length=255, null=True)),
                ('mcq_answer', models.IntegerField(blank=True, null=True)),
                ('multiple_mcq_answer', django.contrib.postgres.fields.ArrayField(base_field=models.IntegerField(), blank=True, null=True, size=None)),
                ('answer_text', models.TextField(blank=True, null=True)),
                ('answer_audio_url', models.URLField(blank=True, max_length=500, null=True)),
                ('status', models.IntegerField(choices=[(0, 'Not Attempted'), (1, 'Attempted'), (2, 'Evaluating'), (3, 'Evaluated')], default=0)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
                ('eval_data', models.JSONField(default=dict, null=True)),
                ('evaluation_id', models.UUIDField(default=uuid.uuid4, editable=False)),
                ('code', models.TextField(blank=True, null=True)),
                ('code_stubs', models.JSONField(blank=True, null=True)),
                ('assessment_attempt_id', models.ForeignKey(default=None, on_delete=django.db.models.deletion.DO_NOTHING, to='evaluation.assessmentattempt')),
                ('question', models.ForeignKey(on_delete=django.db.models.deletion.DO_NOTHING, to='evaluation.question')),
                ('user_id', models.ForeignKey(on_delete=django.db.models.deletion.DO_NOTHING, to=settings.AUTH_USER_MODEL)),
            ],
            options={
                'unique_together': {('user_id', 'question', 'assessment_attempt_id')},
            },
        ),
        migrations.AddConstraint(
            model_name='eventflowprocessorstate',
            constraint=models.UniqueConstraint(fields=('event_flow', 'processor_name'), name='unique_run'),
        ),
        migrations.AlterUniqueTogether(
            name='dsapracticechatdata',
            unique_together={('user_id', 'question', 'assessment_attempt')},
        ),
    ]
